{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import array\n",
    "import numpy as np                                        \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_bin = \"D:/MASS/IOCL/14 inch IPIG ILI run data/Classified data/IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_10_31.07.2015_08.15.30.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_11_31.07.2015_10.37.47.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_1_30.07.2015_10.54.56.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_2_30.07.2015_13.17.12.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_3_30.07.2015_15.39.29.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_4_30.07.2015_18.01.47.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_5_30.07.2015_20.24.04.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_6_30.07.2015_22.46.21.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_7_31.07.2015_01.08.38.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_8_31.07.2015_03.30.56.bin\n",
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_9_31.07.2015_05.53.13.bin\n"
     ]
    }
   ],
   "source": [
    "individual_cd = {}\n",
    "CDs = []\n",
    "pathlist = Path(file_path_bin).glob('**/*.bin')\n",
    "for pathObj in pathlist:\n",
    "    path_in_str = str(pathObj)\n",
    "    print(path_in_str)\n",
    "    cd_num = int(path_in_str.split('\\\\')[-1].split('_')[1])\n",
    "    bin_file_name = path_in_str.split('\\\\')[-1]\n",
    "    CDs.append(cd_num)\n",
    "    individual_cd[cd_num] = path_in_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_10_31.07.2015_08.15.30.bin\n"
     ]
    }
   ],
   "source": [
    "cd_number = 10\n",
    "file_path = individual_cd[cd_number]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'primary_sensors_count' : 84,                             \n",
    "          'secondary_sensors_count' : 72,                              \n",
    "          'other_sensors_count' : 8,\n",
    "          'ADC_bit_size':8,\n",
    "          'pri_skip' : 1,\n",
    "          'sec_skip' : 1,\n",
    "          'other_skip' : 1,\n",
    "          'first_primary_channel' : 0,\n",
    "          'sigma_for_filtering' : 2,\n",
    "          'distance_count' : 5000,\n",
    "          'moving_window_size':150,\n",
    "          'slide_len':10,\n",
    "          'anomaly_threshold':0.25\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 D:\\MASS\\IOCL\\14 inch IPIG ILI run data\\Classified data\\IOCL_PANIPAT_AMBALA_NI DAS_30_31_JULY_2015\\Data\\DATA_10_31.07.2015_08.15.30.bin\n",
      "total_page_count 853\n",
      "image  0  processed\n",
      "image  1  processed\n",
      "image  2  processed\n",
      "image  3  processed\n",
      "image  4  processed\n",
      "image  5  processed\n",
      "image  6  processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7fbdbd5d4c4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindividual_cd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcd_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcd_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcd_anomaly_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manomaly_extractor_from_bin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcd_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcd_anomaly_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path_bin\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcd_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_anomaly.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mrun_anomaly_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrun_anomaly_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcd_anomaly_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-c8228ae2a1ab>\u001b[0m in \u001b[0;36manomaly_extractor_from_bin\u001b[1;34m(cd_num, file_path, params)\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mprimary_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbin_to_decimal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_primary_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_primary_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimary_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;31m#         secondary_data = bin_to_decimal(buffer, last_primary_channel, last_secondary_channel, distance_count, secondary_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-ee69957f09ac>\u001b[0m in \u001b[0;36mbin_to_decimal\u001b[1;34m(buffer, start_chanel, end_chanel, distance_count, numpy_array_to_be_return)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m511\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mnumpy_array_to_be_return\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy_array_to_be_return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_anomaly_df = pd.DataFrame()\n",
    "for cd_num in CDs:\n",
    "    file_path = individual_cd[cd_number]\n",
    "    print(cd_num, file_path)\n",
    "    cd_anomaly_df = anomaly_extractor_from_bin(cd_num, file_path, params)\n",
    "    cd_anomaly_df.to_csv(file_path_bin +'\\\\'+cd_num+'_anomaly.csv')\n",
    "    run_anomaly_df = pd.concat([run_anomaly_df, cd_anomaly_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_number = 10\n",
    "file_path = individual_cd[cd_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_anomaly_df = anomaly_extractor_from_bin(cd_num, file_path, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_10 = cd_anomaly_df\n",
    "print(len(CD_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_CD_10 = remove_duplicate_anomaly(CD_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r unq_CD_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_of_moving_avg</th>\n",
       "      <th>max_of_moving_avg</th>\n",
       "      <th>mean_of_moving_avg</th>\n",
       "      <th>min_of_Max</th>\n",
       "      <th>max_of_Max</th>\n",
       "      <th>mean_of_Max</th>\n",
       "      <th>min_of_Min</th>\n",
       "      <th>max_of_Min</th>\n",
       "      <th>mean_of_Min</th>\n",
       "      <th>min_of_delta</th>\n",
       "      <th>max_of_delta</th>\n",
       "      <th>mean_of_delta</th>\n",
       "      <th>min_of_local_std</th>\n",
       "      <th>max_of_local_std</th>\n",
       "      <th>mean_of_local_std</th>\n",
       "      <th>min_of_global_std</th>\n",
       "      <th>max_of_global_std</th>\n",
       "      <th>mean_of_global_std</th>\n",
       "      <th>min_of_idx_max</th>\n",
       "      <th>max_of_idx_max</th>\n",
       "      <th>mean_of_idx_max</th>\n",
       "      <th>min_of_idx_min</th>\n",
       "      <th>max_of_idx_min</th>\n",
       "      <th>mean_of_idx_min</th>\n",
       "      <th>min_of_idx_delta</th>\n",
       "      <th>max_of_idx_delta</th>\n",
       "      <th>mean_of_idx_delta</th>\n",
       "      <th>min_of_avg</th>\n",
       "      <th>max_of_avg</th>\n",
       "      <th>mean_of_avg</th>\n",
       "      <th>anomalous_count</th>\n",
       "      <th>max_span</th>\n",
       "      <th>start_sid</th>\n",
       "      <th>end_sid</th>\n",
       "      <th>anomaly_start</th>\n",
       "      <th>anomaly_end</th>\n",
       "      <th>anomaly_center</th>\n",
       "      <th>anomaly_length</th>\n",
       "      <th>intensity</th>\n",
       "      <th>anomaly_center1</th>\n",
       "      <th>page_num</th>\n",
       "      <th>abs_idx</th>\n",
       "      <th>cd_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474201</td>\n",
       "      <td>0.921168</td>\n",
       "      <td>0.822928</td>\n",
       "      <td>0.514630</td>\n",
       "      <td>0.933556</td>\n",
       "      <td>0.857321</td>\n",
       "      <td>0.382270</td>\n",
       "      <td>0.916567</td>\n",
       "      <td>0.812816</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.479835</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.118732</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>612</td>\n",
       "      <td>749</td>\n",
       "      <td>662.916667</td>\n",
       "      <td>631</td>\n",
       "      <td>759</td>\n",
       "      <td>696.535714</td>\n",
       "      <td>-144</td>\n",
       "      <td>116</td>\n",
       "      <td>-33.619048</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.919782</td>\n",
       "      <td>0.833206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>612</td>\n",
       "      <td>749</td>\n",
       "      <td>680</td>\n",
       "      <td>137</td>\n",
       "      <td>4.450551</td>\n",
       "      <td>691.7</td>\n",
       "      <td>0</td>\n",
       "      <td>691.7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.504394</td>\n",
       "      <td>0.922339</td>\n",
       "      <td>0.830091</td>\n",
       "      <td>0.515678</td>\n",
       "      <td>0.940002</td>\n",
       "      <td>0.857738</td>\n",
       "      <td>0.382807</td>\n",
       "      <td>0.917095</td>\n",
       "      <td>0.811979</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.412137</td>\n",
       "      <td>0.045760</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.129647</td>\n",
       "      <td>0.013549</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>1264</td>\n",
       "      <td>1386</td>\n",
       "      <td>1271.464286</td>\n",
       "      <td>1242</td>\n",
       "      <td>1375</td>\n",
       "      <td>1304.142857</td>\n",
       "      <td>-109</td>\n",
       "      <td>70</td>\n",
       "      <td>-32.678571</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.919782</td>\n",
       "      <td>0.833206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>1242</td>\n",
       "      <td>1375</td>\n",
       "      <td>1308</td>\n",
       "      <td>133</td>\n",
       "      <td>4.575972</td>\n",
       "      <td>1319.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1319.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503163</td>\n",
       "      <td>0.921322</td>\n",
       "      <td>0.840394</td>\n",
       "      <td>0.512514</td>\n",
       "      <td>0.942335</td>\n",
       "      <td>0.858503</td>\n",
       "      <td>0.386510</td>\n",
       "      <td>0.919261</td>\n",
       "      <td>0.812677</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>0.548130</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.179378</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>2170</td>\n",
       "      <td>2275</td>\n",
       "      <td>2187.130952</td>\n",
       "      <td>2175</td>\n",
       "      <td>2289</td>\n",
       "      <td>2225.726190</td>\n",
       "      <td>-118</td>\n",
       "      <td>100</td>\n",
       "      <td>-38.595238</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.919782</td>\n",
       "      <td>0.833206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>2170</td>\n",
       "      <td>2275</td>\n",
       "      <td>2222</td>\n",
       "      <td>105</td>\n",
       "      <td>4.582611</td>\n",
       "      <td>2224.9</td>\n",
       "      <td>0</td>\n",
       "      <td>2224.9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.505727</td>\n",
       "      <td>0.923521</td>\n",
       "      <td>0.826704</td>\n",
       "      <td>0.512839</td>\n",
       "      <td>0.933838</td>\n",
       "      <td>0.855332</td>\n",
       "      <td>0.382898</td>\n",
       "      <td>0.920011</td>\n",
       "      <td>0.812910</td>\n",
       "      <td>0.011476</td>\n",
       "      <td>0.473417</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.128363</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>2362</td>\n",
       "      <td>2492</td>\n",
       "      <td>2444.845238</td>\n",
       "      <td>2361</td>\n",
       "      <td>2505</td>\n",
       "      <td>2433.130952</td>\n",
       "      <td>-53</td>\n",
       "      <td>95</td>\n",
       "      <td>11.714286</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.919782</td>\n",
       "      <td>0.833206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>2361</td>\n",
       "      <td>2492</td>\n",
       "      <td>2426</td>\n",
       "      <td>131</td>\n",
       "      <td>4.242248</td>\n",
       "      <td>2382.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2382.3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.505473</td>\n",
       "      <td>0.924178</td>\n",
       "      <td>0.826907</td>\n",
       "      <td>0.512603</td>\n",
       "      <td>0.938308</td>\n",
       "      <td>0.853374</td>\n",
       "      <td>0.382217</td>\n",
       "      <td>0.919616</td>\n",
       "      <td>0.812660</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.395114</td>\n",
       "      <td>0.040714</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.138538</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.116224</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>2552</td>\n",
       "      <td>2690</td>\n",
       "      <td>2674.357143</td>\n",
       "      <td>2566</td>\n",
       "      <td>2663</td>\n",
       "      <td>2606.142857</td>\n",
       "      <td>-111</td>\n",
       "      <td>124</td>\n",
       "      <td>68.214286</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.919782</td>\n",
       "      <td>0.833206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>2552</td>\n",
       "      <td>2663</td>\n",
       "      <td>2607</td>\n",
       "      <td>111</td>\n",
       "      <td>4.071369</td>\n",
       "      <td>2607.1</td>\n",
       "      <td>0</td>\n",
       "      <td>2607.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_of_moving_avg  max_of_moving_avg  mean_of_moving_avg  min_of_Max  max_of_Max  mean_of_Max  min_of_Min  max_of_Min  mean_of_Min  min_of_delta  max_of_delta  mean_of_delta  min_of_local_std  max_of_local_std  mean_of_local_std  min_of_global_std  max_of_global_std  mean_of_global_std  min_of_idx_max  max_of_idx_max  mean_of_idx_max  min_of_idx_min  max_of_idx_min  mean_of_idx_min  min_of_idx_delta  max_of_idx_delta  mean_of_idx_delta  min_of_avg  max_of_avg  mean_of_avg  anomalous_count  \\\n",
       "0           0.474201           0.921168            0.822928    0.514630    0.933556     0.857321    0.382270    0.916567     0.812816      0.008934      0.479835       0.044506          0.002266          0.118732           0.011290           0.004551           0.116224            0.015785             612             749       662.916667             631             759       696.535714              -144               116         -33.619048    0.504824    0.919782     0.833206                5   \n",
       "1           0.504394           0.922339            0.830091    0.515678    0.940002     0.857738    0.382807    0.917095     0.811979      0.012794      0.412137       0.045760          0.003494          0.129647           0.013549           0.004551           0.116224            0.015785            1264            1386      1271.464286            1242            1375      1304.142857              -109                70         -32.678571    0.504824    0.919782     0.833206                5   \n",
       "2           0.503163           0.921322            0.840394    0.512514    0.942335     0.858503    0.386510    0.919261     0.812677      0.009456      0.548130       0.045826          0.001729          0.179378           0.013399           0.004551           0.116224            0.015785            2170            2275      2187.130952            2175            2289      2225.726190              -118               100         -38.595238    0.504824    0.919782     0.833206                5   \n",
       "3           0.505727           0.923521            0.826704    0.512839    0.933838     0.855332    0.382898    0.920011     0.812910      0.011476      0.473417       0.042422          0.002348          0.128363           0.011587           0.004551           0.116224            0.015785            2362            2492      2444.845238            2361            2505      2433.130952               -53                95          11.714286    0.504824    0.919782     0.833206                5   \n",
       "4           0.505473           0.924178            0.826907    0.512603    0.938308     0.853374    0.382217    0.919616     0.812660      0.009934      0.395114       0.040714          0.002479          0.138538           0.012953           0.004551           0.116224            0.015785            2552            2690      2674.357143            2566            2663      2606.142857              -111               124          68.214286    0.504824    0.919782     0.833206                5   \n",
       "\n",
       "   max_span  start_sid  end_sid  anomaly_start  anomaly_end  anomaly_center  anomaly_length  intensity  anomaly_center1  page_num  abs_idx  cd_num  \n",
       "0         5         69       74            612          749             680             137   4.450551            691.7         0    691.7      10  \n",
       "1         5         69       74           1242         1375            1308             133   4.575972           1319.5         0   1319.5      10  \n",
       "2         5         69       74           2170         2275            2222             105   4.582611           2224.9         0   2224.9      10  \n",
       "3         5         69       74           2361         2492            2426             131   4.242248           2382.3         0   2382.3      10  \n",
       "4         5         69       74           2552         2663            2607             111   4.071369           2607.1         0   2607.1      10  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_CD_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_extractor_from_bin(cd_num, file_path, params):\n",
    "    #Parse the required values from params dict\n",
    "    primary_sensors_count = params.get('primary_sensors_count')                                \n",
    "    secondary_sensors_count = params.get('secondary_sensors_count')                              \n",
    "    other_sensors_count = params.get('other_sensors_count')\n",
    "    pri_skip = params.get('pri_skip')\n",
    "    sec_skip = params.get('sec_skip')\n",
    "    other_skip = params.get('other_skip')\n",
    "    first_primary_channel = params.get('first_primary_channel')\n",
    "    sigma_for_filtering = params.get('sigma_for_filtering')\n",
    "    distance_count = params.get('distance_count')\n",
    "    moving_window_size = params.get('moving_window_size')\n",
    "    slide_len = params.get('slide_len')\n",
    "    thres = params.get('anomaly_threshold')\n",
    "    adc_bit_size = params.get('ADC_bit_size')\n",
    "    #Getting sensors configuartion\n",
    "    last_primary_channel = primary_sensors_count*pri_skip\n",
    "    last_secondary_channel = last_primary_channel + secondary_sensors_count*sec_skip\n",
    "    last_other_channel = last_secondary_channel  + other_sensors_count*other_skip\n",
    "    max_channels = last_other_channel\n",
    "\n",
    "    #Define required data array\n",
    "    primary_data = np.zeros((primary_sensors_count, distance_count), dtype = np.float)\n",
    "    secondary_data = np.zeros((secondary_sensors_count, distance_count), dtype = np.float)\n",
    "    other_data = np.zeros((other_sensors_count, distance_count), dtype = np.float)\n",
    "    buffer = np.zeros((max_channels, distance_count), dtype = np.float)\n",
    "\n",
    "    cd_anomaly_df = pd.DataFrame()\n",
    "    \n",
    "    fid = open(file_path, 'rb')\n",
    "    arr = array.array(\"b\")\n",
    "    fid.seek(0,0)\n",
    "    EOF = fid.seek(0,2)\n",
    "    total_page_count = int(EOF/(5000*max_channels))\n",
    "    print('total_page_count', total_page_count)\n",
    "    page_num = 0\n",
    "    while (page_num < total_page_count):\n",
    "        fid.seek(page_num*(primary_sensors_count + secondary_sensors_count + other_sensors_count)*distance_count,0)\n",
    "        if(adc_bit_size == 8):\n",
    "            arr = array.array(\"b\")\n",
    "        arr.fromfile(fid, ((max_channels)*distance_count))\n",
    "        print('image ', page_num, ' processed')\n",
    "        k = 0\n",
    "        for iota in range(0, distance_count):\n",
    "            for j in range(0, max_channels):\n",
    "                buffer[j,iota]=arr[k]\n",
    "                k=k+1\n",
    "\n",
    "        primary_data = bin_to_decimal(buffer, first_primary_channel, last_primary_channel, distance_count, primary_data)\n",
    "#         secondary_data = bin_to_decimal(buffer, last_primary_channel, last_secondary_channel, distance_count, secondary_data)\n",
    "\n",
    "        normalized_primary_data = data_normalization(primary_data, 'MinMax')\n",
    "#         normalized_secondary_data = data_normalization(secondary_data, 'MinMax')\n",
    "\n",
    "        filtered_primary_data = gaussian_filter(normalized_primary_data, sigma_for_filtering)\n",
    "#         filtered_secondary_data = gaussian_filter(normalized_secondary_data, sigma_for_filtering)\n",
    "\n",
    "        primary_df = pd.DataFrame(filtered_primary_data)\n",
    "#         secondary_df = pd.DataFrame(filtered_secondary_data)\n",
    "\n",
    "        anomalies = anomaly_scanner(page_num, primary_df.transpose(), moving_window_size, slide_len, thres)\n",
    "        for anomaly in anomalies:\n",
    "            features_dict = feature_generator(cd_num, page_num, anomaly)\n",
    "            features_df = pd.DataFrame(data = features_dict, index = [0])\n",
    "            cd_anomaly_df = pd.concat([cd_anomaly_df, features_df], ignore_index=True)\n",
    "    \n",
    "        page_num = page_num + 1\n",
    "    \n",
    "    fid.close()\n",
    "    return cd_anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_scanner(page_num, page_data, moving_window_size, slide_len, thres = 0.25):\n",
    "#     moving_window_size = 500\n",
    "#     slide_len = 20\n",
    "    indx = 0\n",
    "    shortlisted = []\n",
    "    intensity = 0\n",
    "    center = 4999\n",
    "    i = 0\n",
    "    best_anomaly = pd.DataFrame(columns=['moving_avg', 'Max','Min','delta', 'local_std', 'global_std', 'idx_max', 'idx_min', 'idx_delta', 'avg', 'anomaly'])\n",
    "    while(i < page_data.shape[0]): \n",
    "        global_std = page_data.std()\n",
    "        start = i\n",
    "        end = i + moving_window_size\n",
    "        if(end > 4999):\n",
    "            end = 4999\n",
    "        featured_data = pd.DataFrame(columns=['moving_avg', 'Max','Min','delta', 'local_std', 'global_std', 'idx_max', 'idx_min', 'idx_delta', 'avg', 'anomaly'])\n",
    "        temp = page_data.iloc[start:end,]\n",
    "        #to check wether its same ano maly or differen\n",
    "        featured_data['idx_max'] = temp.idxmax() \n",
    "        featured_data['idx_min'] = temp.idxmin()\n",
    "        delta = temp.max()-temp.min()\n",
    "        local_std = temp.std()\n",
    "        avg_local_std = local_std.mean()\n",
    "        avg_global_std = global_std.mean()\n",
    "\n",
    "        featured_data['moving_avg'] = temp.mean()\n",
    "        featured_data['Max'] = temp.max()\n",
    "        featured_data['Min'] = temp.min()\n",
    "        featured_data['delta'] = featured_data['Max'] - featured_data['Min']\n",
    "        featured_data['local_std'] = local_std\n",
    "        featured_data['global_std'] = global_std\n",
    "        featured_data['idx_delta'] = temp.idxmax() -  temp.idxmin()\n",
    "        featured_data['avg'] = page_data.mean()\n",
    "        featured_data['anomaly'] = featured_data.apply(lambda row : 1 if row.delta > thres else 0, axis=1)\n",
    "        max_span, startSID, endSID = find_max_span(featured_data['anomaly'])\n",
    "        if(max_span > 4  or sum(featured_data['anomaly']) > 10):\n",
    "            max_diff, min_diff = similarity(featured_data, best_anomaly)\n",
    "#             print(i, 'maxDiff = ', max_diff, 'minDiff = ', min_diff)\n",
    "            new_intensity, new_center = anomaly_intensity(featured_data)\n",
    "#             print(i, 'center/new center = ',center, new_center, intensity)\n",
    "            if(len(shortlisted) > 0 and (max_diff < 50 or min_diff < 50 or abs(center - new_center) < 60) ): #assuming Same anomaly \n",
    "                if(new_intensity > intensity):\n",
    "                    shortlisted.pop()\n",
    "#                     print(i, 'Removed previous and added with new intensity = ', new_intensity, intensity)\n",
    "                    intensity = new_intensity\n",
    "                    center = new_center\n",
    "                    best_anomaly = featured_data\n",
    "                    shortlisted.append(best_anomaly)\n",
    "                    \n",
    "#                     print( 'Intensity = ',new_intensity, 'max_span',max_span, 'center = ', new_center)\n",
    "            else:\n",
    "#                 print(i, 'added an anomaly','new_intensity = ', new_intensity, ' center/new_center =>', center, new_center)\n",
    "                intensity = new_intensity\n",
    "                center = new_center\n",
    "                shortlisted.append(featured_data)\n",
    "                \n",
    "                best_anomaly = featured_data\n",
    "            i = i + slide_len\n",
    "        else:\n",
    "            i = i + moving_window_size\n",
    "    \n",
    "    return shortlisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_anomaly(anomalies):\n",
    "    prev_page = -1\n",
    "    drop_list = []\n",
    "    for index, row in anomalies.iterrows(): \n",
    "        page = anomalies.iloc[index]['page_num']\n",
    "        if(page == prev_page):\n",
    "            anomalous_count1 = anomalies.iloc[index-1]['anomalous_count'] \n",
    "            anomalous_count2 = anomalies.iloc[index]['anomalous_count']\n",
    "            anomaly_gap = anomalies.iloc[index]['anomaly_center'] - anomalies.iloc[index-1]['anomaly_center']\n",
    "            anomaly_gap_min = anomalies.iloc[index]['anomaly_start'] - anomalies.iloc[index-1]['anomaly_start']\n",
    "            anomaly_gap_max = anomalies.iloc[index]['anomaly_end'] - anomalies.iloc[index-1]['anomaly_end']\n",
    "            if(anomaly_gap < 100 or anomaly_gap_min < 50 or anomaly_gap_max < 50):\n",
    "#                 print('duplicate found')\n",
    "                if(anomalous_count1 < anomalous_count2):\n",
    "                    drop_list.append(index-1)\n",
    "#                     print('duplicate removed')\n",
    "                else:\n",
    "                    drop_list.append(index)\n",
    "#                     print('duplicate removed')\n",
    "                    \n",
    "        prev_page = page\n",
    "    status = lambda drop_list : print(len(drop_list), ' duplicate anomalies found!') if len(drop_list) > 0 else print('No duplicate anomaly found!')\n",
    "    status(drop_list)\n",
    "    filtered_anomalies = anomalies.drop(drop_list)\n",
    "    filtered_anomalies = filtered_anomalies.reset_index(drop=True)\n",
    "    if(len(drop_list) > 0):\n",
    "        return remove_duplicate_anomaly(filtered_anomalies)\n",
    "    else:\n",
    "        return filtered_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(cd_num, page_num, anomaly_df):\n",
    "    features = {} \n",
    "#     pd.DataFrame(columns=['min_of_moving_avg', 'max_of_moving_avg', 'mean_of_moving_avg', 'min_of_Max', 'max_of_Max', 'mean_of_Max', 'min_of_Min', 'max_of_Min', 'mean_of_Min', 'min_of_delta', 'max_of_delta', 'mean_of_delta', 'min_of_local_std', 'max_of_local_std', 'mean_of_local_std', 'min_of_global_std', 'max_of_global_std', 'mean_of_global_std', 'min_of_idx_delta', 'max_of_idx_delta', 'mean_of_idx_delta', 'min_of_avg', 'max_of_avg', 'mean_of_avg', 'anomalous_count'])\n",
    "    my_list = ['moving_avg', 'Max','Min','delta', 'local_std', 'global_std', 'idx_max', 'idx_min', 'idx_delta', 'avg']\n",
    "    for col_name in anomaly_df.columns :\n",
    "        col_data = anomaly_df[col_name]\n",
    "        if col_name in my_list:\n",
    "            f1_name = 'min_of_' + col_name\n",
    "            f2_name = 'max_of_' + col_name\n",
    "            f3_name = 'mean_of_' + col_name\n",
    "            col_min, col_max, col_mean = min_max_mean(col_data)\n",
    "            features[f1_name] = col_min\n",
    "            features[f2_name] = col_max\n",
    "            features[f3_name] = col_mean\n",
    "        if (col_name == \"anomaly\") :\n",
    "            features['anomalous_count'] = col_data.sum()\n",
    "            max_span, start_sid, end_sid = find_max_span(col_data)\n",
    "            features['max_span'] = max_span\n",
    "            features['start_sid'] = start_sid\n",
    "            features['end_sid'] = end_sid\n",
    "    features['anomaly_start'] = min(features['min_of_idx_min'], features['min_of_idx_max'])\n",
    "    features['anomaly_end'] = min(features['max_of_idx_min'], features['max_of_idx_max'])\n",
    "    features['anomaly_center'] = int((features['anomaly_end'] + features['anomaly_start'])/2)\n",
    "    features['anomaly_length'] = abs(features['anomaly_end']  - features['anomaly_start'])\n",
    "    intensity, center = anomaly_intensity(anomaly_df)\n",
    "    features['intensity'] = intensity\n",
    "    features['anomaly_center1'] = center\n",
    "    features['page_num'] = page_num\n",
    "    features['abs_idx'] = (page_num*5000 + center)\n",
    "    features['cd_num'] = cd_num\n",
    "    return  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_mean(col_data):\n",
    "    col_max = col_data.max()\n",
    "    col_min = col_data.min()\n",
    "    col_mean = col_data.mean()\n",
    "    return col_min, col_max, col_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(featured_data1, featured_data2):\n",
    "    max_diff = 0\n",
    "    min_diff = 0\n",
    "    if(len(featured_data1) > 0 and len(featured_data2) > 0):\n",
    "#         max_span1, startSID1, endSID1 = find_max_span(featured_data1['anomaly'])\n",
    "        max_span2, startSID2, endSID2 = find_max_span(featured_data2['anomaly'])\n",
    "\n",
    "        max_idx1 = featured_data1['idx_max'][startSID2:endSID2]\n",
    "        min_idx1 = featured_data1['idx_min'][startSID2:endSID2]\n",
    "\n",
    "        max_idx2 = featured_data2['idx_max'][startSID2:endSID2]\n",
    "        min_idx2 = featured_data2['idx_min'][startSID2:endSID2]    \n",
    "\n",
    "        max_diff = (sum(max_idx1) - sum(max_idx2))/max_span2\n",
    "        min_diff = (sum(min_idx1) - sum(min_idx2))/max_span2\n",
    "    \n",
    "    return max_diff, min_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_intensity(featured_data):\n",
    "    epsilon = 0.001\n",
    "    anomalous_count = sum(featured_data['anomaly'])\n",
    "    max_span, startSID, endSID = find_max_span(featured_data['anomaly'])\n",
    "    if(anomalous_count == 0):\n",
    "        score = 0\n",
    "        center = 0\n",
    "    else:\n",
    "        delta = sum(featured_data['delta'])/len(featured_data['delta'])\n",
    "        std = ((featured_data['idx_max'][startSID:endSID]).std() +  (featured_data['idx_min'][startSID:endSID]).std()).mean()\n",
    "        score = delta*((max_span+1)/(anomalous_count+1))*100\n",
    "        center =   sum((featured_data['idx_max'][startSID:endSID] +  featured_data['idx_min'][startSID:endSID])/2)/max_span\n",
    "    return score, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_span(col_data):\n",
    "    max_span = 0\n",
    "    span = 0\n",
    "    start_sid = 1000\n",
    "    end_sid = 0\n",
    "    flag = True\n",
    "    for i in range(0, len(col_data)):\n",
    "        data_point = col_data[i]\n",
    "        if(data_point == 1): \n",
    "            span = span + 1\n",
    "        elif (max_span < span):\n",
    "            max_span = span\n",
    "            span = 0\n",
    "            end_sid = i\n",
    "            start_sid = end_sid - max_span\n",
    "            flag = False\n",
    "    if(flag == True): \n",
    "        max_span = span\n",
    "        start_sid = 0\n",
    "        end_sid = len(col_data)\n",
    "        \n",
    "    return max_span, start_sid, end_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator_known(cd_num, page_num, page_data, stride, page_idx, abs_idx, anomaly_type, threshold):\n",
    "    features = {}\n",
    "    funct_start = lambda page_idx, stride : 0 if page_idx < stride else page_idx - stride\n",
    "    funct_end = lambda page_idx, stride : 4999 if stride + page_idx > 4999 else page_idx + stride\n",
    "    start_idx = funct_start(page_idx, stride)\n",
    "    end_idx = funct_end(page_idx, stride)\n",
    "    \n",
    "    featured_data = pd.DataFrame(columns=['moving_avg', 'Max','Min','delta', 'local_std', 'global_std', 'idx_max', 'idx_min', 'idx_delta', 'avg', 'anomaly'])\n",
    "    temp = page_data.iloc[start_idx:end_idx,]\n",
    "    featured_data['idx_max'] = temp.idxmax() \n",
    "    featured_data['idx_min'] = temp.idxmin()\n",
    "    delta = temp.max()-temp.min()\n",
    "    global_std = page_data.std()\n",
    "    local_std = temp.std()\n",
    "    avg_local_std = local_std.mean()\n",
    "    avg_global_std = global_std.mean()\n",
    "\n",
    "    featured_data['moving_avg'] = temp.mean()\n",
    "    featured_data['Max'] = temp.max()\n",
    "    featured_data['Min'] = temp.min()\n",
    "    featured_data['delta'] = featured_data['Max'] - featured_data['Min']\n",
    "    featured_data['local_std'] = local_std\n",
    "    featured_data['global_std'] = global_std\n",
    "    featured_data['idx_delta'] = temp.idxmax() -  temp.idxmin()\n",
    "    featured_data['avg'] = page_data.mean()\n",
    "    featured_data['anomaly'] = featured_data.apply(lambda row : 1 if row.delta > threshold else 0, axis=1)\n",
    "    max_span, startSID, endSID = find_max_span(featured_data['anomaly'])\n",
    "    my_list = ['moving_avg', 'Max','Min','delta', 'local_std', 'global_std', 'idx_max', 'idx_min', 'idx_delta', 'avg']\n",
    "    for col_name in featured_data.columns :\n",
    "        col_data = featured_data[col_name]\n",
    "        if col_name in my_list:\n",
    "            f1_name = 'min_of_' + col_name\n",
    "            f2_name = 'max_of_' + col_name\n",
    "            f3_name = 'mean_of_' + col_name\n",
    "            col_min, col_max, col_mean = min_max_mean(col_data)\n",
    "            features[f1_name] = col_min\n",
    "            features[f2_name] = col_max\n",
    "            features[f3_name] = col_mean\n",
    "        if (col_name == \"anomaly\") :\n",
    "            features['anomalous_count'] = col_data.sum()\n",
    "            max_span, start_sid, end_sid = find_max_span(col_data)\n",
    "            features['max_span'] = max_span\n",
    "            features['start_sid'] = start_sid\n",
    "            features['end_sid'] = end_sid\n",
    "    intensity, center = anomaly_intensity(featured_data)\n",
    "    features['intensity'] = intensity\n",
    "    features['anomaly_start'] = min(features['min_of_idx_min'], features['min_of_idx_max'])\n",
    "    features['anomaly_end'] = min(features['max_of_idx_min'], features['max_of_idx_max'])\n",
    "    features['anomaly_center'] = int((features['anomaly_end'] + features['anomaly_start'])/2)\n",
    "    features['anomaly_center1'] = center\n",
    "    features['anomaly_length'] = abs(features['anomaly_end']  - features['anomaly_start'])\n",
    "    features['page_num'] = page_num\n",
    "    features['page_idx'] = page_idx\n",
    "    features['abs_idx'] = abs_idx\n",
    "    features['anomaly_type'] = anomaly_type\n",
    "    features['cd_num'] = cd_num\n",
    "    return  pd.DataFrame(data = features, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data_array, scaling_method = 'MinMax'):\n",
    "    if(scaling_method == 'MinMax'):\n",
    "        scaler = MinMaxScaler()\n",
    "    if(scaling_method == 'standard'):\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "    normalized_data = np.array(scaler.fit_transform(data_array))\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_decimal(buffer, start_chanel, end_chanel, distance_count, numpy_array_to_be_return):\n",
    "    p1 = 0     \n",
    "    for channel in range(start_chanel, end_chanel):\n",
    "        for samp in range(0, distance_count):\n",
    "            temp = buffer[channel,samp]\n",
    "            temp2 = 511\n",
    "            numpy_array_to_be_return[p1, samp] = ((temp*4) + temp2)*5/2048\n",
    "        p1 = p1 + 1\n",
    "    return numpy_array_to_be_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
